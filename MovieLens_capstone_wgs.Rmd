---
title: "MovieLens Project"
author: "wgstalnaker"  
date: "6/10/2020"
output: pdf_document
---

\pagebreak

## Introduction
<!-- This section that describes the dataset and summarizes the goal of the project and key steps that were performed. -->
The focus of this project will be the development of a movie recommendation system using the MovieLens dataset. The 10M version of the MovieLens dataset will be used to train and test machine learning algorithms to predict movie recommendations. This dataset version contains 10000054 ratings and 95580 tags applied to 10681 movies by 71567 users of the online movie recommendation service MovieLens. Users within this dataset have been selected at random, the only criteria is users have rated at least twenty movies. Additional information related to the MovieLens dataset can be obtained through this link https://group lens.org/data sets/movie lens/10m/. 

Evaluation of algorithms used for movie recommendations will use Root Means Squared Error (RMSE) function to gauge performance. The goal of the recommendation system is to generate prediction where the RMSE is less than 0.86490. Data for the project is generated from a predefined script. The script will create two data partitions edx for training and validation for testing algorithms. For this assignment the validation dataset is not to be used until making final predictions and evaluating the RMSE of the final algorithm. To accommodate this a training and test set will be generated from the edx data as delivered in the data ingestion script to perform training and testing. 

The general approach taken to solve the movie recommendation prediction will be to implement a series of model based approaches. The models will make an assumption that ratings can be generalized across all movies and users with the differences explained as random variations. Model will include ratings related to movies, users, genres and movie released year. This will prove to be an acceptable approach to solving the challenge and meeting the minimum RMSE score. Future analysis of predictions will show a negative impact by large estimate from small samples. This will be handled by implementing regularization to penalize ratings coming large estimates from small samples. Coupling the model based approach and regularization prove effective and allow predictions to be made with minimum time and computer processing power required for such a large dataset. 

\pagebreak

## Methodology 
<!-- This section that explains the process and techniques used, including data cleaning, data exploration and visualization, insights gained, and your modeling approach. -->

### Create Training and Testing Datasets 
The delivered script has been modified slightly to function within an R 4.0 environment
```{r, eval=FALSE}
################################
# Create edx set, validation set
################################
# Note: this process could take a couple of minutes
if(!require(tidyverse)) install.packages("tidy verse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")
# MovieLens 10M dataset:
# https://grouplens.org/datasets/movielens/10m/
# http://files.grouplens.org/datasets/movielens/ml-10m.zip
dl <- tempfile()
download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)
ratings <- fread(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                 col.names = c("userId", "movieId", "rating", "timestamp"))
movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")
# Modification of Ingestion Script to function with R Version 4.0
movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(movieId),
                                           title = as.character(title),
                                           genres = as.character(genres))
movielens <- left_join(ratings, movies, by = "movieId")
# Validation set will be 10% of MovieLens data
set.seed(1, sample.kind="Rounding")
# if using R 3.5 or earlier, use `set.seed(1)` instead
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]
# Make sure userId and movieId in validation set are also in edx set
validation <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")
# Add rows removed from validation set back into edx set
removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)
rm(dl, ratings, movies, test_index, temp, movielens, removed)
````

```{r, include=FALSE}
################################
# Create edx set, validation set
################################
# Note: this process could take a couple of minutes
if(!require(tidyverse)) install.packages("tidy verse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")
# MovieLens 10M dataset:
# https://grouplens.org/datasets/movielens/10m/
# http://files.grouplens.org/datasets/movielens/ml-10m.zip
dl <- tempfile()
download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)
ratings <- fread(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                 col.names = c("userId", "movieId", "rating", "timestamp"))
movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")
# Modification of Ingestion Script to function with R Version 4.0
movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(movieId),
                                           title = as.character(title),
                                           genres = as.character(genres))
movielens <- left_join(ratings, movies, by = "movieId")
# Validation set will be 10% of MovieLens data
set.seed(1, sample.kind="Rounding")
# if using R 3.5 or earlier, use `set.seed(1)` instead
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]
# Make sure userId and movieId in validation set are also in edx set
validation <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")
# Add rows removed from validation set back into edx set
removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)
rm(dl, ratings, movies, test_index, temp, movielens, removed)
```
\pagebreak
### Data Exploration 
MovieLens dataset is delivered in a tidy format, as shown here.
```{r, echo=FALSE}
# MovieLens dataset is in Tidy Format
head(edx)
```
The structure of the dataset is as follows.
```{r, echo=FALSE ,fig.align="left"}
str(edx)
```
Null values do not need to be handled. 
```{r, echo=FALSE}
sapply(edx, {function(x) any(is.na(x))})
```
By summarizing four of the more informative columns of the MovieLens dataset we see the following distinct counts. 
```{r, fig.align="left", echo=FALSE}
# Summarize the edx
# Number of unique user that provided ratings and for a give genre
edx %>% 
  summarize(n_users = n_distinct(userId),
            n_movies = n_distinct(movieId),
            n_genres = n_distinct(genres),
            n_rating = n_distinct(rating))
```
\pagebreak
As previously described the MovieLens dataset contains 10000054 ratings and 95580 tags applied to 10681 movies by 71567 users. When visualizing the rating per movie and per user we get a sense of the spread of data. 

```{r, echo=FALSE, out.width='.49\\linewidth', fig.width=3, fig.height=3, fig.show='hold', fig.align='center'}

# Count of Movie Reviews by User
edx %>% count(movieId) %>% 
  ggplot(aes(n)) + 
  labs(x = "User ID" , y = "Review Count") +
  geom_bar(color="black") + 
  scale_x_log10() + 
  ggtitle("Movie Reviews per User")

# Count of User Reviews by Movie
edx %>% count(userId) %>% 
  ggplot(aes(n)) + 
  labs(x = "Movie ID", y = "Review Count") +
  geom_bar(color="black") + 
  scale_x_log10() + 
  ggtitle("User Reviews per Movie")

```

The following matrix gives much insight into the MovieLens dataset, as it clearly displays the spareness of reviews for a random sample of 100 movies reviews by 100 users. One user has exponentially larger number of ratings for this sample, this could prove insightful in guiding future model builds. The filled matrices show reviewed movies by a user. 

```{r, echo=FALSE, , out.width='.90\\linewidth', fig.show='hold', fig.align='center'}
# Review the sparseness of reviews in the data
# the matrix of a a random sample of 100 movies and 
# 100 users. 
users <- sample(unique(edx$userId), 100)
edx %>% filter(userId %in% users) %>% 
  select(userId, movieId, rating) %>%
  mutate(rating = 1) %>%
  spread(movieId, rating) %>% select(sample(ncol(.), 100)) %>% 
  as.matrix() %>% t(.) %>%
  image(1:100, 1:100,. , xlab="Movies", ylab="Users", col = "black") %>% 
  abline(h=0:100+0.5, v=0:100+0.5, col = "grey")
```

\pagebreak 
Given movies have multiple reviews across independent users will be a key in developing predictive models. Along with what can be inferred from this matrix, we are given two factors for proceeding forward in providing a solution to this challenge

  * Not all movies receive the same number of ratings, therefore we have a Movie Effect.
  * Not all users rate the same number movies, this gives a User Effect.
  
In previous observation of the Genres data in its current format is concerning. If genres are to be used in future models it will need to be flattened to make individual genre types to be accessible. It was also observed that the movie release year is embedded within the title. This data could prove useful but will need to be extracted. The last observation is the current format of the timestamp attribute is not in a useful format. These three observation will be handle prior to beginning modeling.

```{r, echo=FALSE}
##############################################
# Flatten Genre data - During the flattening
# operation the importance of running current
# packages was observered. See discussion post
# https://courses.edx.org/courses/course-v1:HarvardX+PH125.9x+1T2020/discussion/forum/ea27cd383cd33ce9aac90b49d5c06f2680bc38da/threads/5edbdb2a3b7901081c0b2860 To summarize operation perfomance 
# crashing system to complete in <1m
###########################################
edx <- edx %>% separate_rows(genres, sep = "\\|")
validation <- validation %>% separate_rows(genres, sep = "\\|")
###########################################
# Extract year released from title, data is
# in a consistance position making string
# extraction possiable
###########################################
edx <- edx %>% 
  mutate(year_released = as.numeric(str_sub(title,-5, -2)))
validation<- validation %>% 
  mutate(year_released = as.numeric(str_sub(title,-5, -2)))
###########################################
# Format timestamp data
###########################################
edx <- edx %>% 
  mutate(timestamp = as.POSIXct(timestamp, origin = "1960-01-01", tz = "GMT"))
validation <- validation %>% 
  mutate(timestamp = as.POSIXct(timestamp, origin = "1960-01-01", tz = "GMT"))
```

After flatting the genre data we can see the spread across the count of ratings for all users.

```{r, echo=TRUE}
# Count of User Reviews by Genre
edx %>% 
  group_by(genres) %>% 
  summarize(cnt = n(), .groups = 'drop') %>% 
  arrange(desc(cnt))
```

\pagebreak
## Modeling

To begin a baseline of the average movie/user rating will be calculated to evaluate the future Movie Effect and User Effect algorithms against. First we must create the RMSE function to rank future models.

```{r, eval=TRUE}
###################################################################################
# RMSE Function-The root means squared error is defined as n is the number of
# users movie combinations and the sum is occruing over all combinations.
##################################################################################
RMSE <- function(true_ratings, predicted_ratings){
  sqrt(mean((true_ratings - predicted_ratings)^2))
}
```
Second we will be spiting of the edx dataset from the data ingestion script into training and testing partitions with the script.

```{r, echo=TRUE, warning=FALSE}
# Test set will be 10% of edx data from the data ingestion script
set.seed(1, sample.kind="Rounding")
test_index <- createDataPartition(y = edx$rating, times = 1,
                                  p = 0.1, list = FALSE)
train_set <- edx[-test_index,]
temp <- edx[test_index,]
# Make sure userId and movieId in test set are also in training set
test_set <- temp %>% 
     semi_join(train_set, by = "movieId") %>%
     semi_join(train_set, by = "userId")
# Add rows removed from test set back into train set
removed <- anti_join(temp, test_set)
train_set <- rbind(train_set, removed)
rm(test_index, temp, removed)
```

### Baseline Model
<!-- First Model -->
A baseline estimate will be established to benchmark progress of future methodologies to solve the recommendation problem. Previous data exploration points to effects coming form both movies, users, genres, and year of release. To avoid interference the average rating across all movies and users will be the starting baseline. This follows the principle that the estimate that minimizes the root mean squared error is the least squares estimate which is mu. The baseline model is as follows.

```{r, echo=TRUE, include=TRUE}
# Compute the average rating for all movies and users with training set
mu <- mean(train_set$rating) # 3.512574 
# Compute the baseline RMSE on the test dataset
baseline_rmse <- RMSE(test_set$rating, mu) # 1.051741
# Create a table to store the results from different models
rmse_results <- tibble(method = "Baseline Average", RMSE = baseline_rmse)
```
The baseline RSME is 1.05. This results will be stored in a tibble named rmse_results for future reference.
```{r, echo=FALSE, fig.align='left'}
rmse_results %>% knitr::kable()
```

### Movie Effect Model
<!-- second Model -->
For the second model to begin improving the RSME score we will focus on movie reviews. During data exploration we found that some movies were rated more often and received a higher ratings than others, creating a movie effect. To leverage this and enhance the previous model a parameter m_e for the movie effect will be created. The m_e parameter will represent the average rating of movie and will be computed as the average rating minus the average rating of all movies. The movie effect (m_e) model is a follows.

```{r, echo=TRUE, include=TRUE}
# m_e parameter average rating minus the average rating of all movies
movie_avg <- train_set %>% 
  group_by(movieId) %>% 
  summarize(m_e = mean(rating - mu), .groups = 'drop')
```

The results of this new parameter achieve the expected results, a variation of good and bad movies. Recall with the baseline average ~3.5, a m_e of 1.5 would imply the highest rating of 5, for example 3.5 + 1.5 = 5 a perfect rating.

```{r, echo=FALSE, fig.width=4, fig.height=3}
# the results are a mix of good and bad movies as expected. Recall with the
# overall averge ~3.5, a bi of 1.5 would imply a highest rating of 5
qplot(m_e, data = movie_avg, bins = 10, color = I("black"))
```

To make the prediction leveraging the Movie Effect the previous model will be used with the addition of m_e. This will be a representation of the average movie rating as shown below.

```{r, echo=TRUE, include=TRUE}
# Predict using the average rating using the test dataset
predicted_ratings <- mu + test_set %>% 
  left_join(movie_avg, by='movieId') %>%
  .$m_e
# Results for Average using the test dataset
m_e_model_rmse <- RMSE(predicted_ratings, test_set$rating)
# Add the new model results to the RMSE Results table
rmse_results <- bind_rows(rmse_results,
                          tibble(method="Movie Effect Model",
                                     RMSE = m_e_model_rmse ))
```
\pagebreak
Leveraging the movie effect does improve our reduce our root mean squared error. 

```{r, echo=FALSE, , fig.align='left'}
rmse_results %>% knitr::kable()
```

However with closer examination something is a miss with the calculated predictions. The following code creates a dataset for reviewing the top and bottom five movies based on the movie effect model, make note of the cnt column this is sum of reviews per movie.

```{r, include=TRUE}
# dataset of movieId, title, and count of reviews
movie_titles <- train_set %>% 
  select(userId, movieId, title) %>% 
  group_by(movieId, title) %>% 
  summarise(cnt = n_distinct(userId), .groups = 'drop') %>% 
  arrange(desc(cnt))

# 5 best movies based on Movie Effect
movie_avg %>% 
  left_join(movie_titles, by="movieId") %>% 
  arrange(desc(m_e)) %>% 
  select(title, m_e, cnt) %>% 
  slice(1:5) %>% 
  knitr::kable()

# 5 worst based on Movie Effect
movie_avg %>% 
  left_join(movie_titles, by="movieId") %>% 
  arrange(m_e) %>% 
  select(title, m_e, cnt) %>% 
  slice(1:5) %>% 
  knitr::kable()

```
\pagebreak
The predictions are being affected by movies which have high ratings by only a few users to even single users as shown in the cnt variable of sum of ratings per movie. The movies that are listed as the best are very obscure, if they were truly the best movies the number of views and corresponding ratings rate would be much higher. For instances per https://www.imdb.com number 1 movie is the The Godfather (1972) How many reviews does this movie have per IMDB 17747. Per our data 15975.

```{r, include=TRUE}
movie_avg %>% 
  left_join(movie_titles, by="movieId") %>% 
  select(title, m_e, cnt) %>% 
  filter( title == "Godfather, The (1972)") %>% 
  knitr::kable()
```

And the current models best movie is Hellhounds on My Trail wit

```{r, include=TRUE}
movie_avg %>% 
  left_join(movie_titles, by="movieId") %>% 
  select(title, m_e, cnt) %>% 
  filter( title == "Hellhounds on My Trail (1999)") %>% 
  knitr::kable()
```

one single review this clearly justifies implementing regularization into the model. This will allow for a penalty to be placed on large estimates that come from small samples sizes like we see in the case of "Hellhounds on My Trail (1999)" 

### Regularized Movie Effect Model
<!-- Third Model -->

The general goal of this model will be to penalize extreme ratings 5's or 1's which come from only a few or single users. This will be accomplished by creating a parameter to penalize the model when out layers occur. The penalty parameter will be self optimizing to account for those movies which are adversely affecting the RMSE in the previous model. For example "Hellhounds on My Trail (1999)" will have its estimate will be shift toward zero after the penalty has been applied. 

```{r}
# Movie Effect with Regularization 
# using cross validation to select best penalty with a max of 10
penalty <- seq(0, 25, 0.25)
rmses <- sapply(penalty, function(p){
  mu <-mean(train_set$rating)
  # movie effect with regularirzation movies will large estimate 
  # based on small samples will be assigned a penalty
  m_e <- train_set %>%
    group_by(movieId) %>%
    summarize(m_e = sum(rating - mu)/(n()+p), .groups = 'drop')
  predicted_ratings <- test_set %>% 
    left_join(m_e, by = "movieId") %>%
    mutate(pred = mu + m_e) %>%
    .$pred
  
  return(RMSE(predicted_ratings, test_set$rating))
})

rmse_results <- bind_rows(rmse_results,
                          tibble(method="Regularized Movie Effect Model",  
                       RMSE = min(rmses)))
rmse_results %>% knitr::kable()

```

Only a minimal gain is realized however the User Effect still needs to be applied to model. This should allow for addition improvements to the RMSE as previous data exploration indicated much variation in the user rating activity.

### Regularized Movie and User Effect Model
<!-- Forth Model -->

The forth model will determine if user ratings can play a signification factor in predicting a recommendation from the MovieLens dataset. The graph below shows the variation of rating per user. As expected the majority a rated greater than a three, after the all the movie received funding so it must have had some merit. However some are rated significantly higher and lower than average. Does this imply some users more liberal or conservative in their ratings, regularization will be used again to account for this.
```{r, include=TRUE}

# Create a third model to improve RSME score. 
train_set %>% 
group_by(userId) %>% 
  summarize(u_e = mean(rating), .groups = 'drop') %>% 
  filter(n()>=100) %>%
  ggplot(aes(u_e)) + 
  geom_histogram(bins = 30, color = "black")
```

To account for out layer scoring this model will factor in users who rate a movie poorly where the overall user base has rated the movie high. This will be accomplished by creating a u_e parameter for the user effect. This parameter will be the average of the users rating, baseline and movie effect. Once u_e has been calculated it will be used to generate the prediction to be validated against the root mean squared error. We will also include the regularization method used in the previous model, to again account for large estimates based on small sample sizes.

```{r, include=TRUE}
penalty <- seq(0, 25, 0.25)
rmses <- sapply(penalty, function(p){
  mu <-mean(train_set$rating)
  # movie effect with regularirzation
  m_e <- train_set %>%
    group_by(movieId) %>%
    summarize(m_e = sum(rating - mu)/(n()+p), .groups = 'drop')
  #user efect
  u_e <- train_set %>% 
    left_join(m_e, by="movieId") %>%
    group_by(userId) %>%
    summarize(u_e = sum(rating - m_e - mu)/(n()+p), .groups = 'drop')
  # predictions
  predicted_ratings <- test_set %>% 
    left_join(m_e, by = "movieId") %>%
    left_join(u_e, by = "userId") %>%
    mutate(pred = mu + m_e + u_e) %>%
    .$pred
  
  return(RMSE(predicted_ratings, test_set$rating))
})
rmse_results <- bind_rows(rmse_results,
                          tibble(method="Regularized Movie + User Effect Model", RMSE = min(rmses))) 
```

After complete the calculations we do in fact see an improvement and the Regularized Movie + User Effect Model meets the minimum requirement for the project.

```{r, echo=FALSE, fig.align='left'}
rmse_results %>% knitr::kable()
```

To see if the root mean squared error can be reduced even further the same approach will be take as above but introducing both genre and year of release into the model.

```{r, include=TRUE}

penalty <- seq(0, 25, 0.25)
rmses <- sapply(penalty, function(p){
  mu <-mean(train_set$rating)
  # movie effect with regularirzation
  m_e <- train_set %>%
    group_by(movieId) %>%
    summarize(m_e = sum(rating - mu)/(n()+p), .groups = 'drop')
  #user efect
  u_e <- train_set %>% 
    left_join(m_e, by="movieId") %>%
    group_by(userId) %>%
    summarize(u_e = sum(rating - m_e - mu)/(n()+p), .groups = 'drop')
    # genres effect
  g_e <- train_set %>% 
    left_join(m_e, by="movieId") %>%
    left_join(u_e, by="userId") %>% 
    group_by(genres) %>%
    summarize(g_e = sum(rating - m_e - u_e -mu)/(n()+p), .groups = 'drop')
  # predictions
  predicted_ratings <- test_set %>% 
    left_join(m_e, by = "movieId") %>%
    left_join(u_e, by = "userId") %>%
    left_join(g_e, by = "genres") %>% 
    mutate(pred = mu + m_e + u_e + g_e) %>%
    .$pred
  
  return(RMSE(predicted_ratings, test_set$rating))
})
rmse_results <- bind_rows(rmse_results,
                          tibble(method="Regularized Movie + User + Genres Effect Model", RMSE = min(rmses))) 

```

Improvements from Regularized Movie + User + Genres Effect Model 

```{r, echo=FALSE, fig.align='left'}
rmse_results %>% knitr::kable()
```

The last improvement will be including the the year to the movie was released to see if additional gains can be realized. 

```{r, include=TRUE}

penalty <- seq(0, 25, 0.25)
rmses <- sapply(penalty, function(p){
  mu <-mean(train_set$rating)
  # movie effect with regularirzation
  m_e <- train_set %>%
    group_by(movieId) %>%
    summarize(m_e = sum(rating - mu)/(n()+p), .groups = 'drop')
  #user efect
  u_e <- train_set %>% 
    left_join(m_e, by="movieId") %>%
    group_by(userId) %>%
    summarize(u_e = sum(rating - m_e - mu)/(n()+p), .groups = 'drop')
  # genres effect
  g_e <- train_set %>% 
    left_join(m_e, by="movieId") %>%
    left_join(u_e, by="userId") %>% 
    group_by(genres) %>%
    summarize(g_e = sum(rating - m_e - u_e -mu)/(n()+p), .groups = 'drop')
  # year effect
  y_e <- train_set %>% 
    left_join(m_e, by="movieId") %>%
    left_join(u_e, by="userId") %>% 
    left_join(g_e, by="genres") %>% 
    group_by(year_released) %>%
    summarize(y_e = sum(rating - m_e - u_e - g_e - mu)/(n()+p), .groups = 'drop')
  # predictions
  predicted_ratings <- test_set %>% 
    left_join(m_e, by = "movieId") %>%
    left_join(u_e, by = "userId") %>%
    left_join(g_e, by = "genres") %>% 
    left_join(y_e, by = "year_released") %>% 
    mutate(pred = mu + m_e + u_e + g_e + y_e) %>%
    .$pred
  
  return(RMSE(predicted_ratings, test_set$rating))
})
rmse_results <- bind_rows(rmse_results,
                          tibble(method="Regularized Movie + User + Genres + Year Released Effect Model", RMSE = min(rmses))) 

```

Improvements from Regularized Movie + User + Genres + Year Released Effect Model 

```{r, echo=FALSE, fig.align='left'}
rmse_results %>% knitr::kable()
```

## Results 
<!-- This section that presents the modeling results and discusses the model performance. -->

The final model will included movie, user, genre, and year of release effects described above. This model will be trained on the Edx data and it will be tested with the Validation data. 

```{r, include=TRUE}

penalty <- seq(0, 25, 0.25)
final_rmses <- sapply(penalty, function(p){
  mu <-mean(edx$rating)
  # movie effect with regularirzation
  m_e <- edx %>%
    group_by(movieId) %>%
    summarize(m_e = sum(rating - mu)/(n()+p), .groups = 'drop')
  #user efect
  u_e <- edx %>% 
    left_join(m_e, by="movieId") %>%
    group_by(userId) %>%
    summarize(u_e = sum(rating - m_e - mu)/(n()+p), .groups = 'drop')
  # genres effect
  g_e <- edx %>% 
    left_join(m_e, by="movieId") %>%
    left_join(u_e, by="userId") %>% 
    group_by(genres) %>%
    summarize(g_e = sum(rating - m_e - u_e -mu)/(n()+p), .groups = 'drop')
  # year effect
  y_e <- edx %>% 
    left_join(m_e, by="movieId") %>%
    left_join(u_e, by="userId") %>% 
    left_join(g_e, by="genres") %>% 
    group_by(year_released) %>%
    summarize(y_e = sum(rating - m_e - u_e - g_e - mu)/(n()+p), .groups = 'drop')
  # predictions
  predicted_ratings <- validation %>% 
    left_join(m_e, by = "movieId") %>%
    left_join(u_e, by = "userId") %>%
    left_join(g_e, by = "genres") %>% 
    left_join(y_e, by = "year_released") %>% 
    mutate(pred = mu + m_e + u_e + g_e + y_e) %>%
    .$pred
  
  return(RMSE(predicted_ratings, validation$rating))
})

# Create a table to store the final results
final_rmse_results <- tibble(method = "Regularized Movie + User + Genres + Year Released Effect Model" , 
                             RMSE = min(final_rmses))

```

The final score is just under the minimum requirement for this assignment as shown below. This was produced using Regularization on the both the Movie, Genre, Year Releases, User and Year Rated averages.

```{r, echo=FALSE, fig.align='left'}
final_rmse_results %>% knitr::kable()
```

Looking at the top and bottom five movies has improved from previous results. The results are now in alignment with other recommendation application This is a good indicator of the models performance to make accurate predictions, and was made possible by handling the estimates from small samples with regularization.

```{r, echo=FALSE}
# review the results
p <- penalty[which.min(final_rmses)]
# movie effect
m_e <- edx %>%
  group_by(movieId) %>%
  summarize(m_e = sum(rating - mu)/(n()+p), .groups = 'drop')
# user effect
u_e <- edx %>% 
  left_join(m_e, by="movieId") %>%
  group_by(userId) %>%
  summarize(u_e = sum(rating - m_e - mu)/(n()+p), .groups = 'drop')

# Top movies based on Regularized Movie and User Effect
top_5 <- edx %>% 
  count(movieId) %>% 
  left_join(m_e, by = "movieId") %>% 
  left_join(movie_titles, by = "movieId") %>% 
  arrange(desc(m_e)) %>%
  select(title, cnt) %>% 
  slice(1:5) 

# Bottom 5 movies based on Regularized Movie and User Effect
bottom_5 <- edx %>% 
  count(movieId) %>% 
  left_join(m_e, by = "movieId") %>% 
  left_join(movie_titles, by = "movieId") %>% 
  arrange(m_e) %>%
  select(title, cnt) %>% 
  slice(1:5) 
```

Top 5 Movies

```{r, echo=FALSE}
top_5 %>% knitr::kable()
```

Bottom 5 Movies

```{r, echo=FALSE}
bottom_5 %>% knitr::kable()
```

\pagebreak

All scores for each model tested are as follow. 

```{r, echo=FALSE, fig.align='left'}
final_rmse_results %>% knitr::kable()
```

Once the movie and user effects were combined with the use of regularization the minimum required score was obtained and accurate predictions were returned. 

## Conclusion
<!-- This section that gives a brief summary of the report, its limitations and future work. -->

The required RMSE was achieved for this project marking a positive conclusion. The final RMSE generated by the Regularized Movie + Genre + Release Year + User + Rated Year Effect Model was 0.86274 The size of the dataset provided to be the biggest limitation, however once the issue of large estimate from small samples were identified and handle with regularization a basic model based on averages proved successful. Three elements of the MovieLens dataset were utilized by the final model rating, movies, and users. Future models could possible leverage genres and timestamp of the rating to improve the results.

