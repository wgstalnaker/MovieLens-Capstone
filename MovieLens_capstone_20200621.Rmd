---
output: pdf_document
fontsize: 12pt
title: "MovieLens Capstone"
author: "William G. Stalnaker (wgstalnaker)"
date: "June 21, 2020"
---

\pagebreak

\begin{center}
\textbf{Introduction}
\end{center}

This project's focus was the development of a movie recommendation system using the MovieLens data set. The 10M version of the MovieLens data set was used to train and test machine learning algorithms to predict movie recommendations. This data set version of the MovieLens online movie recommendation service contains 10,000,054 ratings and 95,580 tags applied to 10,681 movies by 71,567 users. Users within this data set were selected at random with the only criteria being that users had rated at least 20 movies. Additional information related to the MovieLens data set can be obtained by visiting https://grouplens.org/datasets/movielens/10m/. 

The movie recommendations algorithms used the Root Means Squared Error (RMSE) function to gauge performance. The goal of the recommendation system is to generate predictions where the RMSE is less than 0.86490. Data for the project was generated from a predefined script. The script created two data partitions - edx for training and validation for testing algorithms. For this assignment, the validation data set was not used until making final predictions and evaluating the RMSE of the final algorithm. To accommodate this, both a training and test set were generated from the edx data as delivered in the data ingestion script. 

The general approach taken to solve the movie recommendation prediction was to implement a series of model-based approaches. The models make an assumption that ratings can be generalized across all movies and users with the differences explained as random variations. Models included ratings related to movies, users, genres and release years. The model-based approach proved to be an acceptable solution, but not on its own. Future analysis of predictions showed a negative impact by large estimates from small samples. This negative impact was handled by implementing regularization to penalize ratings coming from large estimates that were derived small samples. Coupling the model-based approach and regularization proved effective and allowed predictions to be made with minimum time and computer processing power required for such a large data set. 

\pagebreak

\begin{center}
\textbf{Methodology}
\end{center}

### Create Training and Testing data sets 

The delivered script has been modified slightly to function within an R 4.0 environment.

```{r, eval=FALSE, warning=FALSE}
################################
# Create edx set, validation set
################################
# Note: this process could take a couple of minutes
if(!require(tidyverse)) install.packages("tidy verse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")
# MovieLens 10M dataset:
# https://grouplens.org/datasets/movielens/10m/
# http://files.grouplens.org/datasets/movielens/ml-10m.zip
dl <- tempfile()
download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)
ratings <- fread(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                 col.names = c("userId", "movieId", "rating", "timestamp"))
movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")
# Modification of Ingestion Script to function with R Version 4.0
movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(movieId),
                                           title = as.character(title),
                                           genres = as.character(genres))
movielens <- left_join(ratings, movies, by = "movieId")
# Validation set will be 10% of MovieLens data
set.seed(1, sample.kind="Rounding")
# if using R 3.5 or earlier, use `set.seed(1)` instead
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]
# Make sure userId and movieId in validation set are also in edx set
validation <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")
# Add rows removed from validation set back into edx set
removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)
rm(dl, ratings, movies, test_index, temp, movielens, removed)
````

```{r, include=FALSE, warning=FALSE}
################################
# Create edx set, validation set
################################
# Note: this process could take a couple of minutes
if(!require(tidyverse)) install.packages("tidy verse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")
# MovieLens 10M dataset:
# https://grouplens.org/datasets/movielens/10m/
# http://files.grouplens.org/datasets/movielens/ml-10m.zip
dl <- tempfile()
download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)
ratings <- fread(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                 col.names = c("userId", "movieId", "rating", "timestamp"))
movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")
# Modification of Ingestion Script to function with R Version 4.0
movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(movieId),
                                           title = as.character(title),
                                           genres = as.character(genres))
movielens <- left_join(ratings, movies, by = "movieId")
# Validation set will be 10% of MovieLens data
set.seed(1, sample.kind="Rounding")
# if using R 3.5 or earlier, use `set.seed(1)` instead
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]
# Make sure userId and movieId in validation set are also in edx set
validation <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")
# Add rows removed from validation set back into edx set
removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)
rm(dl, ratings, movies, test_index, temp, movielens, removed)
```

### Data Exploration 

MovieLens data set is delivered in a tidy format, as shown here:
```{r, echo=FALSE, warning=FALSE}
# MovieLens data set is in Tidy Format
head(edx)
```
The data set structure is as follows:
```{r, echo=FALSE, warning=FALSE}
str(edx)
```
Null values do not need to be handled. 
```{r, echo=FALSE, warning=FALSE}
sapply(edx, {function(x) any(is.na(x))})
```
By summarizing four of the more informative MovieLens data set columns we see the following distinct counts:
```{r, fig.align="left", echo=FALSE, warning=FALSE}
# Summarize the edx
# Number of unique user that provided ratings and for a give genre
edx %>% 
  summarize(n_users = n_distinct(userId),
            n_movies = n_distinct(movieId),
            n_genres = n_distinct(genres),
            n_rating = n_distinct(rating))
```

As previously described, the MovieLens data set contains 10,000,054 ratings and 95,580 tags applied to 10,681 movies by 71,567 users. When visualizing the ratings per movie and per user we get a sense of the data spread: 

**Figure 1**

*Illustration of reviews per movie and user*

```{r, echo=FALSE, out.width='.49\\linewidth', fig.width=3, fig.height=3, fig.show='hold', fig.align='center', warning=FALSE}

# Count of Movie Reviews by User
edx %>% count(movieId) %>% 
  ggplot(aes(n)) + 
  labs(x = "User ID" , y = "Review Count") +
  geom_bar(color="black") + 
  scale_x_log10() + 
  ggtitle("Movie Reviews per User")

# Count of User Reviews by Movie
edx %>% count(userId) %>% 
  ggplot(aes(n)) + 
  labs(x = "Movie ID", y = "Review Count") +
  geom_bar(color="black") + 
  scale_x_log10() + 
  ggtitle("User Reviews per Movie")

```

The following matrix gives much insight into the MovieLens data set, as it clearly displays the sparseness of reviews for a random sample of 100 movie reviews by 100 users. One user in this sample completed an exponentially larger number of reviews, which could prove insightful in guiding future model builds. The filled matrices show reviewed movies by a user. 

**Figure 2**

*Illustration of number of review for a random sample*

```{r, echo=FALSE, , out.width='.90\\linewidth', fig.show='hold', fig.align='center', warning=FALSE}
# Review the sparseness of reviews in the data
# the matrix of a a random sample of 100 movies and 
# 100 users. 
users <- sample(unique(edx$userId), 100)
edx %>% filter(userId %in% users) %>% 
  select(userId, movieId, rating) %>%
  mutate(rating = 1) %>%
  spread(movieId, rating) %>% select(sample(ncol(.), 100)) %>% 
  as.matrix() %>% t(.) %>%
  image(1:100, 1:100,. , xlab="Movies", ylab="Users", col = "black") %>% 
  abline(h=0:100+0.5, v=0:100+0.5, col = "grey")
```

Three key factors in developing the predictive models:

  * Movies have multiple reviews across independent users
  * Not all movies receive the same number of ratings; therefore, we have a movie effect
  * Not all users rate the same number of movies, which gives a user effect

### Data Cleansing  
 
The genres data in its current format is concerning. If genres are to be used in future models, the pipe delimited format will need to be separated to make individual genre types accessible. The movie release year is embedded within the title, which could prove useful but will need to be extracted. These two observations were handled prior to modeling.

```{r, echo=FALSE, warning=FALSE}
##############################################
# Separate Genre data - During the seperation
# the importance of running current
# packages was observered. See discussion post
# https://courses.edx.org/courses/course-v1:HarvardX+PH125.9x+1T2020/discussion/forum/ea27cd383cd33ce9aac90b49d5c06f2680bc38da/threads/5edbdb2a3b7901081c0b2860 To summarize operation perfomance 
# crashing system to complete in <1m
###########################################
edx <- edx %>% separate_rows(genres, sep = "\\|")
validation <- validation %>% separate_rows(genres, sep = "\\|")
###########################################
# Extract year released from title, data is
# in a consistance position making string
# extraction possiable
###########################################
edx <- edx %>% 
  mutate(year_released = as.numeric(str_sub(title,-5, -2)))
validation<- validation %>% 
  mutate(year_released = as.numeric(str_sub(title,-5, -2)))
###########################################
# Format timestamp data
###########################################
edx <- edx %>% 
  mutate(timestamp = as.POSIXct(timestamp, origin = "1960-01-01", tz = "GMT"))
validation <- validation %>% 
  mutate(timestamp = as.POSIXct(timestamp, origin = "1960-01-01", tz = "GMT"))
```

After separating the genre data, we can see the spread across the ratings count for all users.

```{r, echo=TRUE, warning=FALSE}
# Count of User Reviews by Genre
edx %>% 
  group_by(genres) %>% 
  summarize(cnt = n(), .groups = 'drop') %>% 
  arrange(desc(cnt))
```

## Modeling

To begin modeling, a baseline of the average movie/user rating had to be calculated. 

First, the RMSE function was created to rank future models.

```{r, eval=TRUE, warning=FALSE}
###################################################################################
# RMSE Function-The root means squared error is defined as n is the number of
# users movie combinations and the sum is occruing over all combinations.
##################################################################################
RMSE <- function(true_ratings, predicted_ratings){
  sqrt(mean((true_ratings - predicted_ratings)^2))
}
```

Second, the edx data set was split from the data ingestion script, creating both training and testing partitions.

```{r, eval=TRUE, warning=FALSE}
# Test set will be 10% of edx data from the data ingestion script
set.seed(1, sample.kind="Rounding")
test_index <- createDataPartition(y = edx$rating, times = 1,
                                  p = 0.1, list = FALSE)
train_set <- edx[-test_index,]
temp <- edx[test_index,]
# Make sure userId and movieId in test set are also in training set
test_set <- temp %>% 
     semi_join(train_set, by = "movieId") %>%
     semi_join(train_set, by = "userId")
# Add rows removed from test set back into train set
removed <- anti_join(temp, test_set)
train_set <- rbind(train_set, removed)
rm(test_index, temp, removed)
```

### Baseline Model
<!-- First Model -->
A baseline estimate was established to benchmark progress of future methodologies to solve the recommendations problem. Previous data exploration points to effects coming from movies, users, genres, and release years. To avoid interference, the average rating across all movies and users was the starting baseline. This follows the principle that the estimate that minimizes the root mean squared error is the least squares estimate, which is mu. 

The baseline model is as follows:

```{r, echo=TRUE, include=TRUE, warning=FALSE}
# Compute the average rating for all movies and users with training set
mu <- mean(train_set$rating) # 3.512574 
# Compute the baseline RMSE on the test data set
baseline_rmse <- RMSE(test_set$rating, mu) # 1.051741
# Create a table to store the results from different models
rmse_results <- tibble(method = "Baseline Average", RMSE = baseline_rmse)
```

The baseline RMSE is 1.05. This result was stored in a tibble named rmse_results for future reference.

```{r, echo=FALSE, fig.align='left', warning=FALSE}
rmse_results %>% knitr::kable()
```

### Movie Effect Model
<!-- second Model -->
To begin improving the RSME score, the second model focused on movie reviews. The data exploration found that some movies were rated more often and received higher ratings than others, creating a movie effect. To leverage this and enhance the previous model, a parameter m_e for the movie effect was created. The m_e parameter represents the average movie rating and will be computed as the average rating minus the average rating of all movies. 

The movie effect (m_e) model is as follows:

```{r, echo=TRUE, include=TRUE, warning=FALSE}
# m_e parameter average rating minus the average rating of all movies
movie_avg <- train_set %>% 
  group_by(movieId) %>% 
  summarize(m_e = mean(rating - mu), .groups = 'drop')
```

This new parameter achieved the expected results - a variation of both good and bad movies. Recalling with the baseline average ~3.5, a m_e of 1.5 would imply the highest rating of 5. For example: 3.5 + 1.5 = 5, a perfect rating.

**Figure 3**

*Illustration of movie effect ratings*

```{r, echo=FALSE, fig.width=4, fig.height=3, warning=FALSE}
# the results are a mix of good and bad movies as expected. Recall with the
# overall averge ~3.5, a bi of 1.5 would imply a highest rating of 5
qplot(m_e, data = movie_avg, bins = 10, color = I("black"))

```

To build on the previous model the movie effect (m_e) was used. This achieved a representation of the average movie rating as shown below:

```{r, echo=TRUE, include=TRUE, warning=FALSE}
# Predict using the average rating using the test data set
predicted_ratings <- mu + test_set %>% 
  left_join(movie_avg, by='movieId') %>%
  .$m_e
# Results for Average using the test data set
m_e_model_rmse <- RMSE(predicted_ratings, test_set$rating)
# Add the new model results to the RMSE Results table
rmse_results <- bind_rows(rmse_results,
                          tibble(method="Movie Effect Model",
                                     RMSE = m_e_model_rmse ))
```


Leveraging the movie effect does improve the root mean squared error.

```{r, echo=FALSE, , fig.align='left', warning=FALSE}
rmse_results %>% knitr::kable()
```

However, closer examination showed that the calculated predictions were skewed. The following code creates a data set for reviewing the top and bottom five movies based on the movie effect model. Make note of the cnt column - this is the sum of reviews per movie.

```{r, include=TRUE, warning=FALSE}
# data set of movieId, title, and count of reviews
movie_titles <- train_set %>% 
  select(userId, movieId, title) %>% 
  group_by(movieId, title) %>% 
  summarise(cnt = n_distinct(userId), .groups = 'drop') %>% 
  arrange(desc(cnt))

# 5 best movies based on movie effect
movie_avg %>% 
  left_join(movie_titles, by="movieId") %>% 
  arrange(desc(m_e)) %>% 
  select(title, m_e, cnt) %>% 
  slice(1:5) %>% 
  knitr::kable()
```

```{r, include=TRUE, warning=FALSE}
# 5 worst based on movie effect
movie_avg %>% 
  left_join(movie_titles, by="movieId") %>% 
  arrange(m_e) %>% 
  select(title, m_e, cnt) %>% 
  slice(1:5) %>% 
  knitr::kable()
```

In the model's current form, predictions are impacted by movies that have high ratings by only a few users (and even single users), as shown in the cnt variable of sum of ratings per movie. The movies that are listed as the best are very obscure. If they were truly the best movies the number of views and corresponding ratings rate would be much higher. For instance, per https://www.imdb.com, the number one movie is *The Godfather* (1972). IMDB shows that this movie has 17,747 reviews. Conversely, our data shows that this same movie has 17,580 reviews.

```{r, include=TRUE, warning=FALSE}
movie_avg %>% 
  left_join(movie_titles, by="movieId") %>% 
  select(title, m_e, cnt) %>% 
  filter( title == "Godfather, The (1972)") %>% 
  knitr::kable()
```

Additionally, the current model's best movie is *Hellhounds on My Trail* (1999), with only one review.

```{r, include=TRUE, warning=FALSE}
movie_avg %>% 
  left_join(movie_titles, by="movieId") %>% 
  select(title, m_e, cnt) %>% 
  filter( title == "Hellhounds on My Trail (1999)") %>% 
  knitr::kable()
```

These discrepancies clearly justify implementing regularization into the model. This will allow for a penalty to be placed on large estimates that come from small samples sizes like we see in the case of *Hellhounds on My Trail* (1999).

### Regularized Movie Effect Model
<!-- Third Model -->

The general goal of this model was to penalize extreme ratings (5's or 1's) which come from only a few or single users. This was accomplished by creating a parameter to penalize the model when outliers occur. The penalty parameter was self-optimizing to account for those movies which were adversely affecting the RMSE in the previous model. For example, *Hellhounds on My Trail* (1999)'s estimate shifted toward zero after the penalty was applied. 

```{r, warning=FALSE}
# Movie Effect with Regularization 
# using cross validation to select best penalty with a max of 10
penalty <- seq(1, 20, 1)
rmses <- sapply(penalty, function(p){
  mu <-mean(train_set$rating)
  # movie effect with regularirzation movies will large estimate 
  # based on small samples will be assigned a penalty
  m_e <- train_set %>%
    group_by(movieId) %>%
    summarize(m_e = sum(rating - mu)/(n()+p), .groups = 'drop')
  predicted_ratings <- test_set %>% 
    left_join(m_e, by = "movieId") %>%
    mutate(pred = mu + m_e) %>%
    .$pred
  
  return(RMSE(predicted_ratings, test_set$rating))
})

rmse_results <- bind_rows(rmse_results,
                          tibble(method="Regularized Movie Effect Model",  
                       RMSE = min(rmses)))
rmse_results %>% knitr::kable()

```

Only a minimal gain is realized; however, the user effect has not been applied to the current model. This addition allowed for further improvements to the RMSE as previous data exploration indicated much variation in the user rating activity.

### Regularized Movie and User Effect Model
<!-- Forth Model -->

The Regularized Movie + User Effect Model will determine if user ratings were a signification factor in predicting a recommendation from the MovieLens data set. The graph below shows the variations of ratings per user. Some were rated both significantly higher and lower than average. Did this imply that some users were more conservative or liberal in their ratings? Regularization will be used again to account for this.

**Figure 4**

*Illustration of user effect average ratings*

```{r, include=TRUE, warning=FALSE}

# Create a third model to improve RSME score. 
train_set %>% 
group_by(userId) %>% 
  summarize(u_e = mean(rating), .groups = 'drop') %>% 
  filter(n()>=100) %>%
  ggplot(aes(u_e)) + 
  geom_histogram(bins = 30, color = "black")
```

To account for outlier scoring, this model factored in users who rated a movie poorly where the majority user base has rated the movie high. This was accomplished by creating a u_e parameter for the user effect. This parameter averaged the user rating, baseline, and movie effect. Once u_e was calculated it was used to generate the prediction to be validated against the root mean squared error. The regularization method used in the previous model was also included, to again account for large estimates based on small sample sizes.

```{r, include=TRUE, warning=FALSE}
penalty <- seq(1, 20, 1)
rmses <- sapply(penalty, function(p){
  mu <-mean(train_set$rating)
  # movie effect with regularirzation
  m_e <- train_set %>%
    group_by(movieId) %>%
    summarize(m_e = sum(rating - mu)/(n()+p), .groups = 'drop')
  #user efect
  u_e <- train_set %>% 
    left_join(m_e, by="movieId") %>%
    group_by(userId) %>%
    summarize(u_e = sum(rating - m_e - mu)/(n()+p), .groups = 'drop')
  # predictions
  predicted_ratings <- test_set %>% 
    left_join(m_e, by = "movieId") %>%
    left_join(u_e, by = "userId") %>%
    mutate(pred = mu + m_e + u_e) %>%
    .$pred
  
  return(RMSE(predicted_ratings, test_set$rating))
})
rmse_results <- bind_rows(rmse_results,
                          tibble(method="Regularized Movie + User Effect Model", RMSE = min(rmses))) 
```

There is an obvious improvement after completing the calculations and the Regularized Movie + User Effect Model meets the minimum requirement for the project.

```{r, echo=FALSE, fig.align='left', warning=FALSE}
rmse_results %>% knitr::kable()
```

To see if the root mean squared error can be reduced even further the same approach as above was taken. This time, both genre and release year were introduced into the model.

```{r, include=TRUE, warning=FALSE}

penalty <- seq(1, 20, 1)
rmses <- sapply(penalty, function(p){
  mu <-mean(train_set$rating)
  # movie effect with regularirzation
  m_e <- train_set %>%
    group_by(movieId) %>%
    summarize(m_e = sum(rating - mu)/(n()+p), .groups = 'drop')
  #user efect
  u_e <- train_set %>% 
    left_join(m_e, by="movieId") %>%
    group_by(userId) %>%
    summarize(u_e = sum(rating - m_e - mu)/(n()+p), .groups = 'drop')
  # genres effect
  g_e <- train_set %>% 
    left_join(m_e, by="movieId") %>%
    left_join(u_e, by="userId") %>% 
    group_by(genres) %>%
    summarize(g_e = sum(rating - m_e - u_e -mu)/(n()+p), .groups = 'drop')
  # year effect
  y_e <- train_set %>% 
    left_join(m_e, by="movieId") %>%
    left_join(u_e, by="userId") %>% 
    left_join(g_e, by="genres") %>% 
    group_by(year_released) %>%
    summarize(y_e = sum(rating - m_e - u_e - g_e - mu)/(n()+p), .groups = 'drop')
  # predictions
  predicted_ratings <- test_set %>% 
    left_join(m_e, by = "movieId") %>%
    left_join(u_e, by = "userId") %>%
    left_join(g_e, by = "genres") %>% 
    left_join(y_e, by = "year_released") %>% 
    mutate(pred = mu + m_e + u_e + g_e + y_e) %>%
    .$pred
  
  return(RMSE(predicted_ratings, test_set$rating))
})
rmse_results <- bind_rows(rmse_results,
                          tibble(method="Regularized Movie + User + Genres + Year Released Effect Model", RMSE = min(rmses))) 

```

Improvements from Regularized Movie + User + Genres + Year Released Effect Model 

```{r, echo=FALSE, fig.align='left', warning=FALSE}
rmse_results %>% knitr::kable()
```

\begin{center}
\textbf{Results}
\end{center}

The final model included the movie title, user, genre, and release year effects described above. This model was trained on the edx data, and it was tested with the validation data. 

```{r, include=TRUE}

penalty <- seq(10, 20, 0.25)
final_rmses <- sapply(penalty, function(p){
  mu <-mean(edx$rating)
  # movie effect with regularirzation
  m_e <- edx %>%
    group_by(movieId) %>%
    summarize(m_e = sum(rating - mu)/(n()+p), .groups = 'drop')
  #user efect
  u_e <- edx %>% 
    left_join(m_e, by="movieId") %>%
    group_by(userId) %>%
    summarize(u_e = sum(rating - m_e - mu)/(n()+p), .groups = 'drop')
  # genres effect
  g_e <- edx %>% 
    left_join(m_e, by="movieId") %>%
    left_join(u_e, by="userId") %>% 
    group_by(genres) %>%
    summarize(g_e = sum(rating - m_e - u_e -mu)/(n()+p), .groups = 'drop')
  # year effect
  y_e <- edx %>% 
    left_join(m_e, by="movieId") %>%
    left_join(u_e, by="userId") %>% 
    left_join(g_e, by="genres") %>% 
    group_by(year_released) %>%
    summarize(y_e = sum(rating - m_e - u_e - g_e - mu)/(n()+p), .groups = 'drop')
  # predictions
  predicted_ratings <- validation %>% 
    left_join(m_e, by = "movieId") %>%
    left_join(u_e, by = "userId") %>%
    left_join(g_e, by = "genres") %>% 
    left_join(y_e, by = "year_released") %>% 
    mutate(pred = mu + m_e + u_e + g_e + y_e) %>%
    .$pred
  
  return(RMSE(predicted_ratings, validation$rating))
})

# Create a table to store the final results
final_rmse_results <- tibble(method = "Regularized Movie + User + Genres + Year Released Effect Model" , 
                             RMSE = min(final_rmses))

```

The final score was just under the minimum requirement for this assignment, as shown below. This was produced using regularization on the movie, user, genre and release year averages.

```{r, echo=FALSE, fig.align='left', warning=FALSE}
final_rmse_results %>% knitr::kable()
```

Using regularization, the top five and bottom five movies results improved. The results are now in alignment with other movie recommendations platforms. This is a good indicator of the model's prediction accuracy and was made possible by handling the estimates from small samples with regularization.

```{r, echo=FALSE, warning=FALSE}
# review the results
p <- penalty[which.min(final_rmses)]
# movie effect
m_e <- edx %>%
  group_by(movieId) %>%
  summarize(m_e = sum(rating - mu)/(n()+p), .groups = 'drop')
# user effect
u_e <- edx %>% 
  left_join(m_e, by="movieId") %>%
  group_by(userId) %>%
  summarize(u_e = sum(rating - m_e - mu)/(n()+p), .groups = 'drop')

# Top movies based on Regularized Movie and User Effect
top_5 <- edx %>% 
  count(movieId) %>% 
  left_join(m_e, by = "movieId") %>% 
  left_join(movie_titles, by = "movieId") %>% 
  arrange(desc(m_e)) %>%
  select(title, cnt) %>% 
  slice(1:5) 

# Bottom 5 movies based on Regularized Movie and User Effect
bottom_5 <- edx %>% 
  count(movieId) %>% 
  left_join(m_e, by = "movieId") %>% 
  left_join(movie_titles, by = "movieId") %>% 
  arrange(m_e) %>%
  select(title, cnt) %>% 
  slice(1:5) 
```

Top 5 Movies

```{r, echo=FALSE, warning=FALSE}
top_5 %>% knitr::kable()
```

Bottom 5 Movies

```{r, echo=FALSE, warning=FALSE}
bottom_5 %>% knitr::kable()
```

\pagebreak

The final RMSE score is as follows:

```{r, echo=FALSE, fig.align='left', warning=FALSE}
final_rmse_results %>% knitr::kable()
```

Once the movie and user effects were combined with the use of regularization the minimum required score was obtained and accurate predictions were returned. 

\begin{center}
\textbf{Conclusion}
\end{center}

The required RMSE was achieved for this project, marking a positive conclusion. The final RMSE generated by the Regularized Movie + User + Genres + Release Year Effect Model was 0.862. The data set size proved to be the biggest limitation. However, once the issue of large estimates from small samples was identified and handled with regularization, a basic model based on averages proved successful. Four elements of the MovieLens data set were utilized by the final model: movies, users, genres, and release year. Future models could possibly leverage the rating timestamp to improve the results.

